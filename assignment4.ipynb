{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b5a5973-1054-4278-8441-69ff7840f39c",
   "metadata": {},
   "source": [
    "# CSCI 4435/5435: Text Mining & Natural Language Processing\n",
    "## Assignment 4: Recurrent Neural Networks\n",
    "### Student: Miguel Guirao\n",
    "### Aggie ID: 800699208"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a253bda-5f7b-4777-a700-3768d0ee97bd",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Import al requiered libraries\n",
    "- Load the dataset and perform data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "083693cd-1b85-4248-96d4-de85ff8e87fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "import torch\n",
    "import gensim.downloader as api\n",
    "import gensim\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7846cdf-43ed-4395-8f41-6bdb4aa18a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: Python 3.12.3\n",
      "PyTorch version: 2.6.0+cu124\n",
      "Gensim version: 4.3.3\n",
      "Total GPU devices available for use: 1\n",
      "CUDA device name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "Using compute device: cuda:0.\n"
     ]
    }
   ],
   "source": [
    "# Let's setup TensorBoard\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Let's verify some libraries versions\n",
    "# Let's check the versions of our main libraries\n",
    "pyversion = !python --version\n",
    "print(f\"Python version: {pyversion[0]}\\nPyTorch version: {torch.__version__}\\nGensim version: {gensim.__version__}\")\n",
    "\n",
    "# USING GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device_name = torch.cuda.get_device_name()  \n",
    "print(f\"Total GPU devices available for use: {torch.cuda.device_count()}\")\n",
    "print(\"CUDA device name:\", device_name) \n",
    "print(f\"Using compute device: {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36667b48-8ada-45b5-9e85-042476027289",
   "metadata": {},
   "source": [
    "## Load the dataset and perform pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9709928a-4e48-461b-89cc-e0d0b6a2d7a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>link</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>There Were 2 Mass Shootings In Texas Last Week...</td>\n",
       "      <td>Melissa Jeltsen</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/texas-ama...</td>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Will Smith Joins Diplo And Nicky Jam For The 2...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/will-smit...</td>\n",
       "      <td>Of course it has a song.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Hugh Grant Marries For The First Time At Age 57</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hugh-gran...</td>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Jim Carrey Blasts 'Castrato' Adam Schiff And D...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/jim-carre...</td>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Julianna Margulies Uses Donald Trump Poop Bags...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/julianna-...</td>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Morgan Freeman 'Devastated' That Sexual Harass...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/morgan-fr...</td>\n",
       "      <td>\"It is not right to equate horrific incidents ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Donald Trump Is Lovin' New McDonald's Jingle I...</td>\n",
       "      <td>Ron Dicker</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donald-tr...</td>\n",
       "      <td>It's catchy, all right.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>What To Watch On Amazon Prime That’s New This ...</td>\n",
       "      <td>Todd Van Luling</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/amazon-pr...</td>\n",
       "      <td>There's a great mini-series joining this week.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Mike Myers Reveals He'd 'Like To' Do A Fourth ...</td>\n",
       "      <td>Andy McDonald</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/mike-myer...</td>\n",
       "      <td>Myer's kids may be pushing for a new \"Powers\" ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>What To Watch On Hulu That’s New This Week</td>\n",
       "      <td>Todd Van Luling</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/hulu-what...</td>\n",
       "      <td>You're getting a recent Academy Award-winning ...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>Justin Timberlake Visits Texas School Shooting...</td>\n",
       "      <td>Sebastian Murdock</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/justin-ti...</td>\n",
       "      <td>The pop star also wore a \"Santa Fe Strong\" shi...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WORLD NEWS</td>\n",
       "      <td>South Korean President Meets North Korea's Kim...</td>\n",
       "      <td></td>\n",
       "      <td>https://www.huffingtonpost.com/entry/south-kor...</td>\n",
       "      <td>The two met to pave the way for a summit betwe...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>IMPACT</td>\n",
       "      <td>With Its Way Of Life At Risk, This Remote Oyst...</td>\n",
       "      <td>Karen Pinchin</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/remote-oy...</td>\n",
       "      <td>The revolution is coming to rural New Brunswick.</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Trump's Crackdown On Immigrant Parents Puts Mo...</td>\n",
       "      <td>Elise Foley and Roque Planas</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/immigrant...</td>\n",
       "      <td>Last month a Health and Human Services officia...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>POLITICS</td>\n",
       "      <td>'Trump's Son Should Be Concerned': FBI Obtaine...</td>\n",
       "      <td>Michael Isikoff, Yahoo News</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/fbi-wiret...</td>\n",
       "      <td>The wiretaps feature conversations between Ale...</td>\n",
       "      <td>2018-05-26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category                                           headline  \\\n",
       "0           CRIME  There Were 2 Mass Shootings In Texas Last Week...   \n",
       "1   ENTERTAINMENT  Will Smith Joins Diplo And Nicky Jam For The 2...   \n",
       "2   ENTERTAINMENT    Hugh Grant Marries For The First Time At Age 57   \n",
       "3   ENTERTAINMENT  Jim Carrey Blasts 'Castrato' Adam Schiff And D...   \n",
       "4   ENTERTAINMENT  Julianna Margulies Uses Donald Trump Poop Bags...   \n",
       "5   ENTERTAINMENT  Morgan Freeman 'Devastated' That Sexual Harass...   \n",
       "6   ENTERTAINMENT  Donald Trump Is Lovin' New McDonald's Jingle I...   \n",
       "7   ENTERTAINMENT  What To Watch On Amazon Prime That’s New This ...   \n",
       "8   ENTERTAINMENT  Mike Myers Reveals He'd 'Like To' Do A Fourth ...   \n",
       "9   ENTERTAINMENT         What To Watch On Hulu That’s New This Week   \n",
       "10  ENTERTAINMENT  Justin Timberlake Visits Texas School Shooting...   \n",
       "11     WORLD NEWS  South Korean President Meets North Korea's Kim...   \n",
       "12         IMPACT  With Its Way Of Life At Risk, This Remote Oyst...   \n",
       "13       POLITICS  Trump's Crackdown On Immigrant Parents Puts Mo...   \n",
       "14       POLITICS  'Trump's Son Should Be Concerned': FBI Obtaine...   \n",
       "\n",
       "                         authors  \\\n",
       "0                Melissa Jeltsen   \n",
       "1                  Andy McDonald   \n",
       "2                     Ron Dicker   \n",
       "3                     Ron Dicker   \n",
       "4                     Ron Dicker   \n",
       "5                     Ron Dicker   \n",
       "6                     Ron Dicker   \n",
       "7                Todd Van Luling   \n",
       "8                  Andy McDonald   \n",
       "9                Todd Van Luling   \n",
       "10             Sebastian Murdock   \n",
       "11                                 \n",
       "12                 Karen Pinchin   \n",
       "13  Elise Foley and Roque Planas   \n",
       "14   Michael Isikoff, Yahoo News   \n",
       "\n",
       "                                                 link  \\\n",
       "0   https://www.huffingtonpost.com/entry/texas-ama...   \n",
       "1   https://www.huffingtonpost.com/entry/will-smit...   \n",
       "2   https://www.huffingtonpost.com/entry/hugh-gran...   \n",
       "3   https://www.huffingtonpost.com/entry/jim-carre...   \n",
       "4   https://www.huffingtonpost.com/entry/julianna-...   \n",
       "5   https://www.huffingtonpost.com/entry/morgan-fr...   \n",
       "6   https://www.huffingtonpost.com/entry/donald-tr...   \n",
       "7   https://www.huffingtonpost.com/entry/amazon-pr...   \n",
       "8   https://www.huffingtonpost.com/entry/mike-myer...   \n",
       "9   https://www.huffingtonpost.com/entry/hulu-what...   \n",
       "10  https://www.huffingtonpost.com/entry/justin-ti...   \n",
       "11  https://www.huffingtonpost.com/entry/south-kor...   \n",
       "12  https://www.huffingtonpost.com/entry/remote-oy...   \n",
       "13  https://www.huffingtonpost.com/entry/immigrant...   \n",
       "14  https://www.huffingtonpost.com/entry/fbi-wiret...   \n",
       "\n",
       "                                    short_description       date  \n",
       "0   She left her husband. He killed their children... 2018-05-26  \n",
       "1                            Of course it has a song. 2018-05-26  \n",
       "2   The actor and his longtime girlfriend Anna Ebe... 2018-05-26  \n",
       "3   The actor gives Dems an ass-kicking for not fi... 2018-05-26  \n",
       "4   The \"Dietland\" actress said using the bags is ... 2018-05-26  \n",
       "5   \"It is not right to equate horrific incidents ... 2018-05-26  \n",
       "6                             It's catchy, all right. 2018-05-26  \n",
       "7      There's a great mini-series joining this week. 2018-05-26  \n",
       "8   Myer's kids may be pushing for a new \"Powers\" ... 2018-05-26  \n",
       "9   You're getting a recent Academy Award-winning ... 2018-05-26  \n",
       "10  The pop star also wore a \"Santa Fe Strong\" shi... 2018-05-26  \n",
       "11  The two met to pave the way for a summit betwe... 2018-05-26  \n",
       "12   The revolution is coming to rural New Brunswick. 2018-05-26  \n",
       "13  Last month a Health and Human Services officia... 2018-05-26  \n",
       "14  The wiretaps feature conversations between Ale... 2018-05-26  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pandas.pydata.org/docs/reference/api/pandas.read_json.html\n",
    "dataset = pd.read_json(\"dataset/News_Category_Dataset_v2.json\", lines=True) # Read the file as a json object per line.\n",
    "dataset.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb965dff-2c94-4e89-8827-83649d80739b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>She left her husband. He killed their children...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Of course it has a song.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The actor and his longtime girlfriend Anna Ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The actor gives Dems an ass-kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The \"Dietland\" actress said using the bags is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description\n",
       "0  She left her husband. He killed their children...\n",
       "1                           Of course it has a song.\n",
       "2  The actor and his longtime girlfriend Anna Ebe...\n",
       "3  The actor gives Dems an ass-kicking for not fi...\n",
       "4  The \"Dietland\" actress said using the bags is ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop unneeded features\n",
    "dataset.drop(labels=['category', 'headline', 'authors', 'link', 'date'], axis=1, inplace=True)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b5a3d5b-67fc-492c-b211-537961d868d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>short_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>she left her husband he killed their children ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of course it has a song</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the actor and his longtime girlfriend anna ebe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the actor gives dems an ass kicking for not fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the dietland actress said using the bags is a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   short_description\n",
       "0  she left her husband he killed their children ...\n",
       "1                            of course it has a song\n",
       "2  the actor and his longtime girlfriend anna ebe...\n",
       "3  the actor gives dems an ass kicking for not fi...\n",
       "4  the dietland actress said using the bags is a ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loc[:, 'short_description'] = dataset['short_description'].str.lower()\n",
    "dataset.loc[:, 'short_description'] = dataset['short_description'].str.replace(\"-\", \" \", regex=True)\n",
    "dataset.loc[:, 'short_description'] = dataset['short_description'].str.replace(r\"[^'\\&\\w\\s]\", \"\", regex=True)\n",
    "dataset.loc[:, 'short_description'] = dataset['short_description'].str.strip()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77062b32-a6cd-4303-b690-885b961e0189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         she left her husband he killed their children ...\n",
       "1                                   of course it has a song\n",
       "2         the actor and his longtime girlfriend anna ebe...\n",
       "3         the actor gives dems an ass kicking for not fi...\n",
       "4         the dietland actress said using the bags is a ...\n",
       "                                ...                        \n",
       "200848    verizon wireless and at&t are already promotin...\n",
       "200849    afterward azarenka more effusive with the pres...\n",
       "200850    leading up to super bowl xlvi the most talked ...\n",
       "200851    correction an earlier version of this story in...\n",
       "200852    the five time all star center tore into his te...\n",
       "Name: short_description, Length: 200853, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = dataset[\"short_description\"]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7244dd3-8259-41f1-ac3c-2d9bbd7afab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> she left her husband he killed their children just another day in america <end>',\n",
       " '<start> of course it has a song <end>',\n",
       " '<start> the actor and his longtime girlfriend anna eberstein tied the knot in a civil ceremony <end>',\n",
       " '<start> the actor gives dems an ass kicking for not fighting hard enough against donald trump <end>',\n",
       " '<start> the dietland actress said using the bags is a really cathartic therapeutic moment <end>']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = [\" \".join([\"<start>\", x, \"<end>\"]) for x in train_data]\n",
    "# print first row\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bd742-71ad-4323-9cb3-c25e0753c667",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "158b9860-656d-4be2-929f-f0b67df58c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# define Vocab\n",
    "###\n",
    "class Vocab:\n",
    "    def __init__(self, list_of_sentence, tokenization, special_token, max_tokens=None):\n",
    "        # count vocab frequency\n",
    "        vocab_freq = {}\n",
    "        tokens = tokenization(list_of_sentence)\n",
    "        for t in tokens:\n",
    "            for vocab in t:\n",
    "                if vocab not in vocab_freq:\n",
    "                    vocab_freq[vocab] = 0 \n",
    "                vocab_freq[vocab] += 1\n",
    "        # sort by frequency\n",
    "        vocab_freq = {k: v for k, v in sorted(vocab_freq.items(), key=lambda i: i[1], reverse=True)}\n",
    "        # create vocab list\n",
    "        self.vocabs = [special_token] + list(vocab_freq.keys())\n",
    "        if max_tokens:\n",
    "            self.vocabs = self.vocabs[:max_tokens]\n",
    "        self.stoi = {v: i for i, v in enumerate(self.vocabs)}\n",
    "\n",
    "    def _get_tokens(self, list_of_sentence):\n",
    "        for sentence in list_of_sentence:\n",
    "            tokens = tokenizer.tokenize(sentence)\n",
    "            yield tokens\n",
    "\n",
    "    def get_itos(self):\n",
    "        return self.vocabs\n",
    "\n",
    "    def get_stoi(self):\n",
    "        return self.stoi\n",
    "\n",
    "    def append_token(self, token):\n",
    "        self.vocabs.append(token)\n",
    "        self.stoi = {v: i for i, v in enumerate(self.vocabs)}\n",
    "\n",
    "    def __call__(self, list_of_tokens):\n",
    "        def get_token_index(token):\n",
    "            if token in self.stoi:\n",
    "                return self.stoi[token]\n",
    "            else:\n",
    "                return 0\n",
    "        return [get_token_index(t) for t in list_of_tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe8eb7ca-2480-4a24-96ff-13d0c9c391c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# generate Vocab\n",
    "###\n",
    "max_word = 50000\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = SpaceTokenizer()\n",
    "\n",
    "# define tokenization function\n",
    "def yield_tokens(data):\n",
    "    for text in data:\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        yield tokens\n",
    "\n",
    "# build vocabulary list, of size max_word\n",
    "vocab = Vocab(train_data, tokenization=yield_tokens, special_token=\"<unk>\", max_tokens=max_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24a910cf-ee45-4eee-989e-0d01c3e865a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_index: 50000, vocab size: 50001\n"
     ]
    }
   ],
   "source": [
    "pad_index = vocab.__len__()\n",
    "vocab.append_token(\"<pad>\")\n",
    "print(f\"pad_index: {pad_index}, vocab size: {len(vocab.vocabs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46c375a9-3ead-4f03-a420-0881db9b1eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of token index is 50001.\n",
      "The padded index is 50000.\n"
     ]
    }
   ],
   "source": [
    "itos = vocab.get_itos()\n",
    "stoi = vocab.get_stoi()\n",
    "# test\n",
    "print(\"The number of token index is {}.\".format(vocab.__len__()))\n",
    "print(\"The padded index is {}.\".format(stoi[\"<pad>\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73c3c280-1d8f-4e70-8fc1-032ba2f34117",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 256\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, feature_list = [], []\n",
    "    for text in batch:\n",
    "        # tokenize to a list of word's indices\n",
    "        tokens = vocab(tokenizer.tokenize(text))\n",
    "        # separate into features and labels\n",
    "        y = tokens[1:]\n",
    "        y.append(-100)\n",
    "        x = tokens\n",
    "        # limit length to max_seq_len\n",
    "        y = y[:max_seq_len]\n",
    "        x = x[:max_seq_len]\n",
    "        # pad features and labels\n",
    "        y += [-100] * (max_seq_len - len(y))\n",
    "        x += [pad_index] * (max_seq_len - len(x))\n",
    "        # add to list\n",
    "        label_list.append(y)\n",
    "        feature_list.append(x)\n",
    "    # convert to tensor\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64).to(device)\n",
    "    feature_list = torch.tensor(feature_list, dtype=torch.int64).to(device)\n",
    "    return label_list, feature_list\n",
    "\n",
    "dataloader = DataLoader(train_data, batch_size=32, shuffle=True, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02536b15-7c9a-4c3f-aa24-a636b858ee98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label shape in batch : torch.Size([32, 256])\n",
      "feature shape in batch : torch.Size([32, 256])\n",
      "\n",
      "***** label sample *****\n",
      "tensor([  39,   12,   14,   39,   12,   26,  446,  859,    2, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100], device='cuda:0') \n",
      "\n",
      "***** features sample *****\n",
      "tensor([    1,    39,    12,    14,    39,    12,    26,   446,   859,     2,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000, 50000,\n",
      "        50000, 50000, 50000, 50000, 50000, 50000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "for labels, features in dataloader:\n",
    "    break\n",
    "\n",
    "print(\"label shape in batch : {}\".format(labels.size()))\n",
    "print(\"feature shape in batch : {}\\n\".format(features.size()))\n",
    "print(\"***** label sample *****\")\n",
    "print(labels[0], \"\\n\")\n",
    "print(\"***** features sample *****\")\n",
    "print(features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7c922-3330-45da-80d5-52b662649882",
   "metadata": {},
   "source": [
    "## Embeddings: Option A with a pre-trained Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "176a93af-7032-43bc-b0b0-6f8c3a92ba7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32cb49fd-85a5-44fe-bef9-c518a305bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_vec = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99efbe61-9ee5-4da3-a5a3-c1e301384f64",
   "metadata": {},
   "source": [
    "# Task 1: Language Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94374b86-6ee2-4eba-8760-833ad4d5b44d",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f796d1d2-c11c-4bcf-acb9-16f44a25f662",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 64\n",
    "rnn_units = 512\n",
    "\n",
    "class SimpleRnnModel(nn.Module):\n",
    "    def __init__(self, vocab_size, seq_len, embedding_dim, rnn_units, padding_idx):\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        self.embedding = nn.Embedding(\n",
    "            vocab_size,\n",
    "            embedding_dim,\n",
    "            padding_idx=padding_idx,\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=rnn_units,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.classify = nn.Linear(rnn_units, vocab_size)\n",
    "\n",
    "    def forward(self, inputs, states=None, return_final_state=False):\n",
    "        # embedding\n",
    "        #   --> (batch_size, seq_len, embedding_dim)\n",
    "        outs = self.embedding(inputs)\n",
    "        # build \"lengths\" property to pack inputs (see above)\n",
    "        lengths = (inputs != self.padding_idx).int().sum(dim=1, keepdim=False)\n",
    "        # pack inputs for RNN\n",
    "        packed_inputs = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            outs,\n",
    "            lengths.cpu(),\n",
    "            batch_first=True,\n",
    "            enforce_sorted=False,\n",
    "        )\n",
    "        # apply RNN\n",
    "        if states is None:\n",
    "            packed_outs, final_state = self.rnn(packed_inputs)\n",
    "        else:\n",
    "            packed_outs, final_state = self.rnn(packed_inputs, states)\n",
    "        # unpack results\n",
    "        #   --> (batch_size, seq_len, rnn_units)\n",
    "        outs, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outs,\n",
    "            batch_first=True,\n",
    "            padding_value=0.0,\n",
    "            total_length=self.seq_len,\n",
    "        )\n",
    "        # apply feed-forward to classify\n",
    "        #   --> (batch_size, seq_len, vocab_size)\n",
    "        logits = self.classify(outs)\n",
    "        # return results\n",
    "        if return_final_state:\n",
    "            return logits, final_state  # This is used in prediction\n",
    "        else:\n",
    "            return logits               # This is used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98a77e19-53f4-42b7-be12-e3b5a1f577a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRNN = SimpleRnnModel(vocab_size=vocab.__len__(), seq_len=max_seq_len, embedding_dim=embedding_dim, rnn_units=rnn_units, padding_idx=pad_index).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "641aef55-c988-4f27-b7ce-554e2fd14fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 3.69 GiB of which 375.12 MiB is free. Including non-PyTorch memory, this process has 3.30 GiB memory in use. Of the allocated memory 3.19 GiB is allocated by PyTorch, and 13.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m optimizer.zero_grad()\n\u001b[32m     10\u001b[39m logits = modelRNN(seqs)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m loss = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m loss.backward()\n\u001b[32m     13\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/venvs/nlp/lib/python3.12/site-packages/torch/nn/functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 1.53 GiB. GPU 0 has a total capacity of 3.69 GiB of which 375.12 MiB is free. Including non-PyTorch memory, this process has 3.30 GiB memory in use. Of the allocated memory 3.19 GiB is allocated by PyTorch, and 13.74 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "optimizer = torch.optim.AdamW(modelRNN.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for labels, seqs in dataloader:\n",
    "        # optimize\n",
    "        optimizer.zero_grad()\n",
    "        logits = modelRNN(seqs)\n",
    "        loss = F.cross_entropy(logits.transpose(1,2), labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # calculate accuracy\n",
    "        pred_labels = logits.argmax(dim=2)\n",
    "        num_correct = (pred_labels == labels).float().sum()\n",
    "        num_total = (labels != -100).float().sum()\n",
    "        accuracy = num_correct / num_total\n",
    "        print(\"Epoch {} - loss: {:2.4f} - accuracy: {:2.4f}\".format(epoch+1, loss.item(), accuracy), end=\"\\r\")\n",
    "    writer.add_scalar('Loss/train_rnn', loss.item(), epoch+1)\n",
    "    writer.add_scalar('Accuracy/train_rnn', accuracy, epoch+1)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe28e5-e32d-4e9e-b8f3-3b46fcbee7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c96836c-f4d2-4911-8082-2af57264b774",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a0710-55fc-4249-9fdf-77df1caa4ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
